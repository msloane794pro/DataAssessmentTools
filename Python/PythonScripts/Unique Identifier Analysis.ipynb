{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Identifier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypyodbc \n",
    "import pandas as pd\n",
    "import ast\n",
    "import glob\n",
    "import textwrap\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "import warnings\n",
    "import re\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER = 'SERVERNAME'\n",
    "DATABASE = 'DATABASE_NAME'\n",
    "connectionString = f'DRIVER={{SQL Server Native Client 11.0}};SERVER={SERVER};DATABASE={DATABASE};TRUSTED_CONNECTION=yes'\n",
    "cnxn = pypyodbc.connect(connectionString)\n",
    "type = 'view' ## 'table' or 'view'\n",
    "qualification = DATABASE+'.dbo.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_string(sql_file, encoding='utf-16'):\n",
    "    with open(sql_file, 'r', encoding=encoding) as f_in:\n",
    "        lines = f_in.read()\n",
    "        # remove common leading whitespace from all lines    \n",
    "        query_string = textwrap.dedent(\"\"\"{}\"\"\".format(lines))\n",
    "        return query_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Primary Keys\n",
    "\n",
    "Find columns which do not have duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Populate list of tables/views to be assessed\n",
    "table_list = ['table1','table2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueness_check(tablename):\n",
    "    unique_cols = []\n",
    "    ## return max number of duplicate UID values\n",
    "    data_df = pd.read_sql_query('Select * from '+tablename, cnxn)\n",
    "    for column in data_df.columns:\n",
    "        obj_count = data_df.groupby(column).size().max()        \n",
    "        if obj_count == 1:\n",
    "            with open(uq_path, 'a') as f:\n",
    "                f.write(tablename+'\\t'+column+'\\n')\n",
    "            print(column + ' in '+tablename+' is unique')\n",
    "            unique_cols.append(column)\n",
    "    return data_df, unique_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LOAD PREVIOUS##\n",
    "uq_path = DATABASE+'_UniqueColumns.csv'\n",
    "if os.path.isfile(uq_path):\n",
    "    unique_cols = pd.read_csv(uq_path, names=['tablename','columnname'], sep='\\t')\n",
    "\n",
    "for tablename in table_list:\n",
    "    if tablename not in unique_cols['tablename'].values:\n",
    "        data_df, u_cols = uniqueness_check(tablename)\n",
    "\n",
    "##LOAD UPDATED##\n",
    "unique_cols = pd.read_csv(uq_path, names=['tablename','columnname'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover composite keys\n",
    "\n",
    "Discover which tables have duplicate values for candidate unique identifier (UID) field(s)\n",
    "\n",
    "For those tables, which other fields have unique values for the same UID\n",
    "\n",
    "For those fields, which vary together\n",
    "\n",
    "Validate final composite key choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Give the names and suggested primary key for the tables to be analyzed for composite keys\n",
    "## aka, those that did not return any viable unique columns\n",
    "cpk_dict = {'table1':'id1'\n",
    "            ,'table2':'id1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpk_fields(data_df, tablename, idname):\n",
    "    # ## Find fields with unique values for the same UID\n",
    "    max_uniques = data_df.groupby(idname).nunique().max()\n",
    "    non_unique_cols = max_uniques[max_uniques>1].index.tolist()\n",
    "    with open(nuc_path, 'a') as f:\n",
    "        f.write(tablename+'\\t'+str(non_unique_cols)+'\\n')\n",
    "    return non_unique_cols\n",
    "\n",
    "def analyze_cpks(data_df, tablename, idname, groupcol):\n",
    "    ## Which composite key candidate fields always match values and which don't \n",
    "    col_results_df = pd.DataFrame(columns=['Table','ID1','ID2','Unique Column','Unique Count'])\n",
    "    for uniquecol in data_df.columns:\n",
    "        if (groupcol.lower() == uniquecol) or (idname == uniquecol):\n",
    "            continue\n",
    "        count = data_df.groupby([idname.lower(), groupcol.lower()])[uniquecol].nunique().max()\n",
    "        with open(cka_path, 'a') as f:\n",
    "            f.write(tablename+'\\t'+idname+'\\t'+groupcol+'\\t'+uniquecol+'\\t'+str(count)+'\\n')\n",
    "            col_results_df.loc[len(col_results_df)] = {'Table':tablename,'ID1':idname,'ID2':groupcol,'Unique Column':uniquecol,'Unique Count':count}\n",
    "    return col_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LOAD PREVIOUS##\n",
    "nuc_path = DATABASE+'_CompositeKeyCandidates.csv'\n",
    "cka_path = DATABASE+'_CompositeKeyAnalysis.csv'\n",
    "if os.path.isfile(nuc_path):\n",
    "    non_unique_cols_df = pd.read_csv(nuc_path, names=['tablename','non-unique columns'], sep='\\t')\n",
    "if os.path.isfile(cka_path):\n",
    "    cpk_results_df = pd.read_csv(cka_path, header=None, names= ['Table', 'ID1','ID2','Unique Column','Unique Count'], sep='\\t')\n",
    "\n",
    "for tablename, idname in cpk_dict.items():\n",
    "    idname = idname.lower()\n",
    "    if tablename not in non_unique_cols_df['tablename'].values:\n",
    "        cols_to_analyze = cpk_fields(data_df, tablename, idname)\n",
    "        for col in cols_to_analyze:\n",
    "            result_df = analyze_cpks(data_df, tablename, idname, col)\n",
    "\n",
    "##LOAD Updated##\n",
    "non_unique_cols_df = pd.read_csv(nuc_path, names=['tablename','non-unique columns'], sep='\\t')\n",
    "cpk_results_df = pd.read_csv(cka_path, header=None, names= ['tablename', 'ID1','ID2','Unique Column','Unique Count'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_candidates = pd.DataFrame(cpk_results_df.groupby(['tablename','ID1','ID2'])['Unique Count'].max()).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_cpks = double_candidates[double_candidates['Unique Count']<2]\n",
    "## TODO: Pick one option for ID2 for each table\n",
    "double_cpks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For those that did not produce viable second IDs, analyze for 3 or more part composite keys\n",
    "multi_candidates = pd.DataFrame(cpk_results_df[~cpk_results_df['tablename'].isin(double_cpks['tablename'].values.tolist())].groupby(['tablename','ID1','ID2'])['Unique Count'].max()).reset_index()\n",
    "multi_candidates = multi_candidates[multi_candidates['Unique Count']>1]\n",
    "multi_candidate_fields = cpk_results_df[(cpk_results_df['tablename'].isin(multi_candidates['tablename'].values.tolist()))&(cpk_results_df['ID2'].isin(multi_candidates['ID2'].values.tolist()))&(cpk_results_df['Unique Count']>1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: select groups of IDs which appear to always vary together\n",
    "multi_candidate_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Input final choices for composite key\n",
    "val_cpk_dict = {\n",
    "    'table1':['id1','id2']\n",
    "    ,'table2':['id1','id2','id3']\n",
    "}\n",
    "## Validate uniqueness\n",
    "def cpk_validate(tablename, col_list):\n",
    "    col_string = ', '.join(col_list)\n",
    "    query_string = 'SELECT max(a.objectCount) from (SELECT count(*) as objectCount FROM '+tablename+' GROUP BY '+col_string+' )a'\n",
    "    count = pd.read_sql_query(query_string, cnxn).values[0][0]\n",
    "    if count > 1:\n",
    "        print('Please examine choice of keys for ', tablename)\n",
    "\n",
    "for tablename, comp_key_list in val_cpk_dict.items():    \n",
    "    cpk_validate(tablename, comp_key_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
