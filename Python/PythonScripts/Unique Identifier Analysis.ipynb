{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Identifier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypyodbc \n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER = 'SERVERNAME'\n",
    "DATABASE = 'databasename'\n",
    "connectionString = f'DRIVER={{SQL Server Native Client 11.0}};SERVER={SERVER};DATABASE={DATABASE};TRUSTED_CONNECTION=yes'\n",
    "cnxn = pypyodbc.connect(connectionString)\n",
    "type = 'view' ## 'table' or 'view'\n",
    "qualification = DATABASE+'.dbo.'\n",
    "\n",
    "save_path = 'path\\\\to\\\\where\\\\you\\\\want\\\\results\\\\saved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_string(sql_file, encoding='utf-16'):\n",
    "    with open(sql_file, 'r', encoding=encoding) as f_in:\n",
    "        lines = f_in.read()\n",
    "        # remove common leading whitespace from all lines    \n",
    "        query_string = textwrap.dedent(\"\"\"{}\"\"\".format(lines))\n",
    "        return query_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Primary Keys\n",
    "\n",
    "Find columns which do not have duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Populate list of tables/views to be assessed with proposed unique id\n",
    "tables_dict = {\n",
    "'tablename':'uid'\n",
    ",'tablename2':'uid'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueness_check(uq_file, tablename):\n",
    "    unique_cols = []\n",
    "    ## return max number of duplicate UID values\n",
    "    data_df = pd.read_sql_query('Select * from '+tablename, cnxn)\n",
    "    for column in data_df.columns:\n",
    "        obj_count = data_df.groupby(column).size().max()       \n",
    "        if obj_count == 1:\n",
    "            with open(uq_file, 'a') as f:\n",
    "                f.write(tablename+','+column+'\\n')\n",
    "            unique_cols.append(column)\n",
    "    return data_df, unique_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LOAD PREVIOUS##\n",
    "uq_file = save_path+ DATABASE+'_UniqueColumns.csv'\n",
    "cpk_file = save_path+ DATABASE+'_CompositeKeyTables.txt'\n",
    "if os.path.isfile(uq_file):\n",
    "    unique_cols = pd.read_csv(uq_file, names=['tablename','columnname'], sep=',')\n",
    "else:\n",
    "    unique_cols = pd.DataFrame(columns=['tablename','columnname'])\n",
    "\n",
    "if os.path.isfile(cpk_file):\n",
    "    with open(cpk_file, 'r') as f:\n",
    "        cpk_list = f.read().splitlines()\n",
    "else:\n",
    "    cpk_list = []\n",
    "\n",
    "## Update with new results\n",
    "for tablename in tables_dict.keys():\n",
    "    if tablename not in unique_cols['tablename'].values and tablename not in cpk_list:\n",
    "        data_df, u_cols = uniqueness_check(uq_file, tablename)\n",
    "        if len(u_cols) == 0:\n",
    "            cpk_list.append(tablename)\n",
    "            with open(cpk_file, 'a') as f:\n",
    "                f.write('\\n'+tablename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##LOAD UPDATED##\n",
    "unique_cols = pd.read_csv(uq_file, names=['tablename','columnname'], sep='\\t')\n",
    "with open(cpk_file, 'r') as f:\n",
    "    cpk_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Pick one UID for each table with unique columns\n",
    "unique_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover composite keys\n",
    "\n",
    "Discover which tables have duplicate values for candidate unique identifier (UID) field(s)\n",
    "\n",
    "For those tables, which other fields have unique values for the same UID\n",
    "\n",
    "For those fields, which vary together\n",
    "\n",
    "Validate final composite key choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ckc_fields(ckc_file, data_df, tablename, idname):\n",
    "    # ## Find fields with unique values for the same UID\n",
    "    max_uniques = data_df.groupby(idname).nunique().max()\n",
    "    non_unique_cols = max_uniques[max_uniques>1].index.tolist()\n",
    "    # non_unique_cols_string = '\\',\\''.join(non_unique_cols)\n",
    "    with open(ckc_file, 'a') as f:\n",
    "        f.write(tablename+'\\t'+str(non_unique_cols)+'\\n') #XXX fix this\n",
    "    return non_unique_cols\n",
    "\n",
    "def analyze_cpks(cka_file, data_df, tablename, idname, groupcol):\n",
    "    ## Which composite key candidate fields always match values and which don't \n",
    "    col_results_df = pd.DataFrame(columns=['Table','ID1','ID2','Unique Column','Unique Count'])\n",
    "    for uniquecol in data_df.columns:\n",
    "        if (groupcol.lower() == uniquecol) or (idname == uniquecol):\n",
    "            continue\n",
    "        count = data_df.groupby([idname.lower(), groupcol.lower()])[uniquecol].nunique().max()\n",
    "        with open(cka_file, 'a') as f:\n",
    "            f.write(tablename+','+idname+','+groupcol+','+uniquecol+','+str(count)+'\\n') #XXX fix this\n",
    "            col_results_df.loc[len(col_results_df)] = {'Table':tablename,'ID1':idname,'ID2':groupcol,'Unique Column':uniquecol,'Unique Count':count}\n",
    "    return col_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get names and suggested primary key for the tables to be analyzed for composite keys\n",
    "## aka, those that did not return any viable unique columns\n",
    "cpk_dict = {key: value for key, value in tables_dict.items() if key in cpk_list}\n",
    "\n",
    "##LOAD PREVIOUS##\n",
    "ckc_file = save_path+DATABASE+'_CompositeKeyCandidates.csv'\n",
    "cka_file = save_path+DATABASE+'_CompositeKeyAnalysis.csv'\n",
    "\n",
    "if os.path.isfile(ckc_file):\n",
    "    non_unique_cols_df = pd.read_csv(ckc_file, names=['tablename','non-unique columns'], sep='\\t')\n",
    "else:\n",
    "    non_unique_cols_df = pd.DataFrame(columns=['tablename','non-unique columns'])\n",
    "if os.path.isfile(cka_file):\n",
    "    cpk_results_df = pd.read_csv(cka_file, header=None, names= ['Table', 'ID1','ID2','Unique Column','Unique Count'], sep=',')\n",
    "else:\n",
    "    cpk_results_df = pd.DataFrame(columns=['Table', 'ID1','ID2','Unique Column','Unique Count'])\n",
    "\n",
    "for tablename, idname in cpk_dict.items():\n",
    "    idname = idname.lower()\n",
    "    if tablename not in non_unique_cols_df['tablename'].values:\n",
    "        print('Analyzing '+ tablename + ' for non-unique columns')\n",
    "        data_df = pd.read_sql_query('Select * from '+tablename, cnxn)\n",
    "        cols_to_analyze = ckc_fields(ckc_file, data_df, tablename, idname)\n",
    "        for col in cols_to_analyze:\n",
    "            result_df = analyze_cpks(cka_file, data_df, tablename, idname, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##LOAD Updated##\n",
    "non_unique_cols_df = pd.read_csv(save_path+DATABASE+'_CompositeKeyCandidates.csv', names=['tablename','non-unique columns'], sep='\\t')\n",
    "cpk_results_df = pd.read_csv(cka_file,names=['Table', 'ID1','ID2','Unique Column','Unique Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_unique_cols_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpk_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_candidates = pd.DataFrame(cpk_results_df.groupby(['Table','ID1','ID2'])['Unique Count'].max())#.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_cpks = double_candidates[double_candidates['Unique Count']<2]\n",
    "## TODO: Pick an option for ID2 from each table\n",
    "double_cpks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For those that did not produce viable second IDs, analyze for 3 or more part composite keys\n",
    "multi_candidates = pd.DataFrame(cpk_results_df[~cpk_results_df['Table'].isin(double_cpks.reset_index()['Table'].values.tolist())].groupby(['Table','ID1','ID2'])['Unique Count'].max()).reset_index()\n",
    "multi_candidates = multi_candidates[multi_candidates['Unique Count']>1]\n",
    "multi_candidate_fields = cpk_results_df[(cpk_results_df['Table'].isin(multi_candidates['Table'].values.tolist()))&(cpk_results_df['ID2'].isin(multi_candidates['ID2'].values.tolist()))&(cpk_results_df['Unique Count']>1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: select one column from each group of columns which appear to always vary together to be ID2 (ID3, ID4...)\n",
    "multi_candidate_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Input final choices for composite key\n",
    "val_cpk_dict = {\n",
    "    'table1':['id1','id2']\n",
    "    ,'table2':['id1','id2','id3']\n",
    "}\n",
    "\n",
    "## Validate uniqueness\n",
    "def cpk_validate(tablename, col_list):\n",
    "    col_string = ', '.join(col_list)\n",
    "    query_string = 'SELECT max(a.objectCount) from (SELECT count(*) as objectCount FROM '+tablename+' GROUP BY '+col_string+' )a'\n",
    "    count = pd.read_sql_query(query_string, cnxn).values[0][0]\n",
    "    if count > 1:\n",
    "        print('Please examine choice of keys for ', tablename)\n",
    "\n",
    "for tablename, comp_key_list in val_cpk_dict.items():    \n",
    "    cpk_validate(tablename, comp_key_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
